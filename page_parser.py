import httpx
import asyncio
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import os
import csv
import json
import re
from config import TRUSTED_DOMAINS, FAKE_DOMAINS, PLATFORM_DOMAINS, CLICKBAIT_TRIGGERS
from loguru import logger
import dateparser
from newspaper import Article
import google.generativeai as genai
from rich.console import Console
from rich.panel import Panel
from rich.text import Text
from rich.table import Table
from rich import box

console = Console()
genai.configure(api_key=os.getenv("GEMINI_KEY"))

def print_rich_card(item: dict):
    title = item.get('title') or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
    url = item.get('url')
    rating = item.get('rating', '')
    date = item.get('published_date') or "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    ai_text = item.get('ai_analysis')

    ai_score_display = ""
    if ai_text and "–ü—Ä–æ–ø—É—â–µ–Ω–æ" not in ai_text and "–∫–æ—Ä–æ—Ç–∫–∏–π" not in ai_text:
        # –ò—â–µ–º "85%", "100%", "5%" –≤ —Ç–µ–∫—Å—Ç–µ
        match = re.search(r'SCORE:\s*(\d{1,3})%', ai_text, re.IGNORECASE)
        if match:
            score = int(match.group(1))
            if score >= 80:
                ai_score_display = f" | [bold green]AI: {score}%[/bold green]"
            elif score >= 50:
                ai_score_display = f" | [bold yellow]AI: {score}%[/bold yellow]"
            else:
                ai_score_display = f" | [bold red]AI: {score}%[/bold red]"
            ai_text = re.sub(r'SCORE:\s*\d{1,3}%\s*', '', ai_text, flags=re.IGNORECASE).strip()

    border_style = "white"
    if "–í—ã—Å–æ–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ" in rating:
        border_style = "green"
    elif "–ù–∏–∑–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ" in rating or "–ü—Ä–æ–ø–∞–≥–∞–Ω–¥–∞" in rating:
        border_style = "red"
    elif "–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞" in rating:
        border_style = "yellow"

    content = Table.grid(padding=(0, 1))
    content.add_column(style="bold white", justify="right")
    content.add_column(style="white")

    content.add_row("üìÖ –î–∞—Ç–∞:", f"[cyan]{date}[/cyan]")
    content.add_row("üîó URL:", f"[blue underline]{url}[/blue underline]")

    rating_colored = rating \
        .replace("–í—ã—Å–æ–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ", "[bold green]–í—ã—Å–æ–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ[/bold green]") \
        .replace("–ü—Ä–æ–ø–∞–≥–∞–Ω–¥–∞", "[bold red]–ü—Ä–æ–ø–∞–≥–∞–Ω–¥–∞[/bold red]") \
        .replace("–ù–∏–∑–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ", "[bold red]–ù–∏–∑–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ[/bold red]") \
        .replace("–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞", "[yellow]–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞[/yellow]") \
        .replace("–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω", "[grey70]–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω[/grey70]")

    content.add_row("üõ°Ô∏è –†–µ–π—Ç–∏–Ω–≥:", f"{rating_colored}{ai_score_display}")

    if ai_text and "–ü—Ä–æ–ø—É—â–µ–Ω–æ" not in ai_text and "–∫–æ—Ä–æ—Ç–∫–∏–π" not in ai_text:
        clean_text = ai_text.replace("**", "").replace("###", "").replace("\n", " ").strip()
        ai_preview = clean_text[:150] + "..."
        content.add_row("ü§ñ –ú–Ω–µ–Ω–∏–µ:", f"[italic grey70]{ai_preview}[/italic grey70]")

    panel = Panel(
        content,
        title=f"[bold]{title}[/bold]",
        subtitle=f"[dim]–ò—Å—Ç–æ—á–Ω–∏–∫: {urlparse(url).netloc}[/dim]",
        border_style=border_style,
        box=box.ROUNDED,
        expand=False
    )

    console.print(panel)

async def get_ai_analyzis(text: str) -> str:
    if not text or len(text) < 100:
        return None
    model = genai.GenerativeModel('gemini-2.5-flash')
    prompt = f"""
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Ç–µ–∫—Å—Ç.
    –í–ê–ñ–ù–û: –¢–≤–æ–π –æ—Ç–≤–µ—Ç –û–ë–Ø–ó–ê–ù –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å—Ç—Ä–æ–≥–æ —Å —Ç–∞–∫–æ–π —Å—Ç—Ä–æ–∫–∏:
    SCORE: [—á–∏—Å–ª–æ –æ—Ç 0 –¥–æ 100]%
    –î–∞–ª–µ–µ –Ω–∞–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑:
    1. –ü—Ä–∏—á–∏–Ω—ã –æ—Ü–µ–Ω–∫–∏.
    2. –ü—Ä–∏–∑–Ω–∞–∫–∏ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π (–µ—Å–ª–∏ –µ—Å—Ç—å).
    3. –í–µ—Ä–¥–∏–∫—Ç (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).

    –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏:
    "{text[:3000]}"
    """

    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = await model.generate_content_async(prompt)
            return response.text
        except Exception as e:
            if "429" in str(e):
                wait_time = 20 + (attempt * 10)
                logger.warning(f"–õ–∏–º–∏—Ç API (429). –ñ–¥—É {wait_time} —Å–µ–∫ –∏ –ø—Ä–æ–±—É—é —Å–Ω–æ–≤–∞...")
                await asyncio.sleep(wait_time)
            else:
                logger.error(f"Gemini Error: {e}")
                return f"–û—à–∏–±–∫–∞ AI: {e}"
    return "–û—à–∏–±–∫–∞ AI: –õ–∏–º–∏—Ç –∏—Å—á–µ—Ä–ø–∞–Ω –ø–æ—Å–ª–µ 3 –ø–æ–ø—ã—Ç–æ–∫"

def get_domain_rating(url):
    try:
        domain = urlparse(url).netloc
        if domain.startswith('www.'):
            domain = domain[4:]

        if domain in TRUSTED_DOMAINS:
            return "–†–µ–π—Ç–∏–Ω–≥: –í—ã—Å–æ–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ"
        if domain in FAKE_DOMAINS:
            return "–†–µ–π—Ç–∏–Ω–≥: –ù–∏–∑–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ / –ü—Ä–æ–ø–∞–≥–∞–Ω–¥–∞"
        if domain in PLATFORM_DOMAINS:
            return "–†–µ–π—Ç–∏–Ω–≥: –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ (–ù–µ –°–ú–ò)"

        return "–†–µ–π—Ç–∏–Ω–≥: –ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω"
    except Exception:
        return "–†–µ–π—Ç–∏–Ω–≥: –û—à–∏–±–∫–∞ (–Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π URL)"

def extract_date(soup):
    time_tag = soup.find("time")
    if time_tag and time_tag.has_attr("datetime"):
        return time_tag["datetime"]

    meta_properties = [
        "article:published_time",
        "datePublished",
        "og:updated_time",
        "og:published_time",
        "pubdate"
    ]
    for prop in meta_properties:
        meta_tag = soup.find("meta", {"property": prop}) or soup.find("meta", {"name": prop})
        if meta_tag and meta_tag.has_attr("content"):
            return meta_tag["content"]
    return None

def save_report(report_data, show_logs: bool):
    if not report_data:
        if show_logs:
            logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞.")
        return
    try:
        with open("report.json", "w", encoding="utf-8") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=4)
        if show_logs:
            logger.success("–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ report.json")
    except Exception as e:
        if show_logs:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")
        else:
            console.print(f"[red]‚ùå –û—à–∏–±–∫–∞ JSON: {e}[/red]")

    try:
        fieldnames = report_data[0].keys()
        with open("report.csv", "w", newline='', encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(report_data)
        if show_logs:
            logger.success("–§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ report.csv")
        else:
            console.print(f"\n[bold green]üíæ –û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: report.json, report.csv[/bold green]")
    except Exception as e:
        if show_logs:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è CSV: {e}")
        else:
            console.print(f"[red]‚ùå –û—à–∏–±–∫–∞ CSV: {e}[/red]")

def analyze_title_sentiment(title: str | None) -> str:
    if not title:
        return ""
    title_lower = title.lower()
    for trigger in CLICKBAIT_TRIGGERS:
        if trigger in title_lower:
            logger.debug(f"–ù–∞–π–¥–µ–Ω–æ —Ç—Ä–∏–≥–≥–µ—Ä-—Å–ª–æ–≤–æ: '{trigger}'")
            return " (–ö–ª–∏–∫–±–µ–π—Ç: –¢—Ä–∏–≥–≥–µ—Ä-—Å–ª–æ–≤–æ)"
    if '!!!' in title or '???' in title or '!?' in title:
        logger.debug("–ù–∞–π–¥–µ–Ω–∞ —á—Ä–µ–∑–º–µ—Ä–Ω–∞—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è")
        return " (–ö–ª–∏–∫–±–µ–π—Ç: –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è)"
    if title.isupper() and len(title) > 10:
        logger.debug("–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞–ø–∏—Å–∞–Ω –≤ ALL CAPS")
        return " (–ö–ª–∏–∫–±–µ–π—Ç: ALL CAPS)"
    return ""

async def fetch_and_parse_url(client: httpx.AsyncClient, url: str, semaphore: asyncio.Semaphore, show_logs: bool) -> dict:
    async with semaphore:
        if show_logs:
            logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {url}")
        else:
            console.print(f"[grey50]‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞: {urlparse(url).netloc}...[/grey50]")
        domain_rating = get_domain_rating(url)
        report_item = {
            'url': url,
            'title': None,
            'published_date': None,
            'rating': domain_rating,
            'status': 'Failed', # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
            'ai_analysis': None
        }
        ai_score_short = ""
        try:
            response = await client.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "lxml")
            article = Article(url)
            article.set_html(response.text)
            article.parse()

            if "youtube.com" in url or "youtu.be" in url:
                report_item['ai_analysis'] = "–ü—Ä–æ–ø—É—â–µ–Ω–æ (–í–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç)"
                if show_logs: logger.info("AI –ø—Ä–æ–ø—É—â–µ–Ω (YouTube)")
            elif not article.text or len(article.text) < 100:
                report_item['ai_analysis'] = "–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
                if show_logs: logger.warning("AI –ø—Ä–æ–ø—É—â–µ–Ω (–ú–∞–ª–æ —Ç–µ–∫—Å—Ç–∞)")
            else:
                if show_logs: logger.info("–û—Ç–ø—Ä–∞–≤–ª—è—é —Ç–µ–∫—Å—Ç –≤ AI...")
                ai_result = await get_ai_analyzis(article.text)
                if ai_result:
                    report_item['ai_analysis'] = ai_result
                    if show_logs: logger.success("AI –∞–Ω–∞–ª–∏–∑ –ø–æ–ª—É—á–µ–Ω!")
                    match = re.search(r'(\d{1,3}%)', ai_result)
                    if match:
                        ai_score_short = f" | AI: {match.group(1)}"
                    await asyncio.sleep(2)

            title = article.title
            if not title:
                title_tag = soup.find("title")
                h1_tag = soup.find("h1")
                title = title_tag.text.strip() if title_tag else (h1_tag.text.strip() if h1_tag else None)
            if title and show_logs:
                logger.success(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {title}")
            else:
                if show_logs: logger.warning("–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

            report_item['title'] = title
            sentiment_tag = analyze_title_sentiment(title)
            final_rating = f"{domain_rating}{sentiment_tag}{ai_score_short}"

            publish_date = article.publish_date
            if publish_date:
                iso_date = publish_date.isoformat()
                if show_logs: logger.success(f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {iso_date}")
                report_item['published_date'] = iso_date
            else:
                if show_logs: logger.debug("–ü—Ä–æ–±—É—é –Ω–∞–π—Ç–∏ –¥–∞—Ç—É –≤—Ä—É—á–Ω—É—é")
                raw_date_str = extract_date(soup)
                if raw_date_str:
                    parsed_date = dateparser.parse(str(raw_date_str))
                    if parsed_date:
                        iso_date = parsed_date.isoformat()
                        if show_logs: logger.success(f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {iso_date}")
                        report_item['published_date'] = iso_date
                    else:
                        if show_logs: logger.warning(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞ –¥–∞—Ç—ã, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å: {raw_date_str}")
                else:
                    if show_logs: logger.warning("–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            report_item['status'] = 'Success'
            if show_logs:
                logger.success(f"{final_rating}")
                if report_item['published_date']:
                    logger.success(f"–î–∞—Ç–∞: {report_item['published_date']}")
                print("\n")
            else:
                print_rich_card(report_item)

        except Exception as e:
            if show_logs:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url}: {e}")
            else:
                console.print(f"[red]‚ùå –û—à–∏–±–∫–∞ {urlparse(url).netloc}: {e}[/red]")
            report_item['status'] = f"Failed: {e}"
            print("\n")

        return report_item

async def run_parser(search_results_data, show_logs: bool):
    links = [item["link"] for item in search_results_data.get("items", [])]

    headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
    'Accept-Encoding': 'gzip, deflate, br',
    'DNT': '1',
    'Upgrade-Insecure-Requests': '1',
}

    semaphore = asyncio.Semaphore(3)
    tasks = []
    if not show_logs:
        console.print(f"[bold cyan]üöÄ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è {len(links)} —Å—Å—ã–ª–æ–∫...[/bold cyan]\n")
    async with httpx.AsyncClient(headers=headers, follow_redirects=True) as client:
        for url in links:
            tasks.append(fetch_and_parse_url(client, url, semaphore, show_logs))
        if show_logs: logger.info(f"–ó–∞–ø—É—Å–∫–∞—é {len(tasks)} –∑–∞–¥–∞—á –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ...")
        final_report_data = await asyncio.gather(*tasks)

    save_report(final_report_data, show_logs)
