import httpx
import asyncio
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import os
import csv
import json
import re
from config import TRUSTED_DOMAINS, FAKE_DOMAINS, PLATFORM_DOMAINS, CLICKBAIT_TRIGGERS
from database import DatabaseHandler
from loguru import logger
import dateparser
from typing import Optional
from newspaper import Article
from google import genai
from rich.console import Console
from rich.panel import Panel
from rich.text import Text
from rich.table import Table
from rich import box
import datetime

console = Console()

def print_rich_card(item: dict):
    title = item.get('title') or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
    url = item.get('url')
    rating = item.get('rating', '')
    date = item.get('published_date') or "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    ai_text = item.get('ai_analysis')

    border_style = "white"
    if "–í—ã—Å–æ–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ" in rating:
        border_style = "green"
    elif "–ù–∏–∑–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ" in rating or "–ü—Ä–æ–ø–∞–≥–∞–Ω–¥–∞" in rating:
        border_style = "red"
    elif "–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞" in rating:
        border_style = "yellow"

    content = Table.grid(padding=(0, 1))
    content.add_column(style="bold white", justify="right")
    content.add_column(style="white")

    content.add_row("üìÖ –î–∞—Ç–∞:", f"[cyan]{date}[/cyan]")
    content.add_row("üîó URL:", f"[blue underline]{url}[/blue underline]")

    rating_colored = rating \
        .replace("–í—ã—Å–æ–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ", "[bold green]–í—ã—Å–æ–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ[/bold green]") \
        .replace("–ü—Ä–æ–ø–∞–≥–∞–Ω–¥–∞", "[bold red]–ü—Ä–æ–ø–∞–≥–∞–Ω–¥–∞[/bold red]") \
        .replace("–ù–∏–∑–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ", "[bold red]–ù–∏–∑–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ[/bold red]") \
        .replace("–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞", "[yellow]–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞[/yellow]") \
        .replace("–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω", "[grey70]–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω[/grey70]")

    def color_ai_score(match):
        score = int(match.group(1))
        if score >= 80: color = "bold green"
        elif score >= 50: color = "bold yellow"
        else: color = "bold red"
        return f"| [{color}]AI: {score}%[/{color}]"

    rating_colored = re.sub(r'\|\s*AI:\s*(\d{1,3})%', color_ai_score, rating_colored)

    content.add_row("üõ°Ô∏è –†–µ–π—Ç–∏–Ω–≥:", f"{rating_colored}")

    if ai_text and "–ü—Ä–æ–ø—É—â–µ–Ω–æ" not in ai_text and "–∫–æ—Ä–æ—Ç–∫–∏–π" not in ai_text:
        clean_text = re.sub(r'SCORE:\s*\d{1,3}%\s*', '', ai_text, flags=re.IGNORECASE)
        clean_text = ai_text.replace("**", "").replace("###", "").replace("\n", " ").strip()
        ai_preview = clean_text[:150] + "..."
        content.add_row("ü§ñ –ú–Ω–µ–Ω–∏–µ:", f"[italic grey70]{ai_preview}[/italic grey70]")

    panel = Panel(
        content,
        title=f"[bold]{title}[/bold]",
        subtitle=f"[dim]–ò—Å—Ç–æ—á–Ω–∏–∫: {urlparse(url).netloc}[/dim]",
        border_style=border_style,
        box=box.ROUNDED,
        expand=False
    )

    console.print(panel)

async def get_ai_analyzis(text: str) -> Optional[str]:
    if not text or len(text) < 100:
        return None
    # model = genai.GenerativeModel('gemini-2.5-flash')
    prompt = f"""
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Ç–µ–∫—Å—Ç.
    –í–ê–ñ–ù–û: –¢–≤–æ–π –æ—Ç–≤–µ—Ç –û–ë–Ø–ó–ê–ù –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å—Ç—Ä–æ–≥–æ —Å —Ç–∞–∫–æ–π —Å—Ç—Ä–æ–∫–∏:
    SCORE: [—á–∏—Å–ª–æ –æ—Ç 0 –¥–æ 100]%
    –î–∞–ª–µ–µ –Ω–∞–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑:
    1. –ü—Ä–∏—á–∏–Ω—ã –æ—Ü–µ–Ω–∫–∏.
    2. –ü—Ä–∏–∑–Ω–∞–∫–∏ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π (–µ—Å–ª–∏ –µ—Å—Ç—å).
    3. –í–µ—Ä–¥–∏–∫—Ç (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).

    –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏:
    "{text[:3000]}"
    """

    max_retries = 3
    for attempt in range(max_retries):
        try:
            client = genai.Client(api_key=os.getenv("GEMINI_KEY"))
            response = await client.aio.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt)
            return response.text
        except Exception as e:
            if "429" in str(e):
                wait_time = 20 + (attempt * 10)
                logger.warning(f"–õ–∏–º–∏—Ç API (429). –ñ–¥—É {wait_time} —Å–µ–∫ –∏ –ø—Ä–æ–±—É—é —Å–Ω–æ–≤–∞...")
                await asyncio.sleep(wait_time)
            else:
                logger.error(f"Gemini Error: {e}")
                return f"–û—à–∏–±–∫–∞ AI: {e}"
    return "–û—à–∏–±–∫–∞ AI: –õ–∏–º–∏—Ç –∏—Å—á–µ—Ä–ø–∞–Ω –ø–æ—Å–ª–µ 3 –ø–æ–ø—ã—Ç–æ–∫"

def get_domain_rating(url):
    try:
        domain = urlparse(url).netloc
        if domain.startswith('www.'):
            domain = domain[4:]

        if domain in TRUSTED_DOMAINS:
            return "–†–µ–π—Ç–∏–Ω–≥: –í—ã—Å–æ–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ"
        if domain in FAKE_DOMAINS:
            return "–†–µ–π—Ç–∏–Ω–≥: –ù–∏–∑–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ / –ü—Ä–æ–ø–∞–≥–∞–Ω–¥–∞"
        if domain in PLATFORM_DOMAINS:
            return "–†–µ–π—Ç–∏–Ω–≥: –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ (–ù–µ –°–ú–ò)"

        return "–†–µ–π—Ç–∏–Ω–≥: –ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω"
    except Exception:
        return "–†–µ–π—Ç–∏–Ω–≥: –û—à–∏–±–∫–∞ (–Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π URL)"

def extract_date(soup):
    time_tag = soup.find("time")
    if time_tag and time_tag.has_attr("datetime"):
        return time_tag["datetime"]

    meta_properties = [
        "article:published_time",
        "datePublished",
        "og:updated_time",
        "og:published_time",
        "pubdate"
    ]
    for prop in meta_properties:
        meta_tag = soup.find("meta", {"property": prop}) or soup.find("meta", {"name": prop})
        if meta_tag and meta_tag.has_attr("content"):
            return meta_tag["content"]
    return None

def save_report(report_data: list, query: str, show_logs: bool):
    if not report_data: return

    db = DatabaseHandler()
    saved_count = 0

    for item in report_data:
        if item and item.get('status') != 'Failed':
            if db.save_article(item, query):
                saved_count += 1

    if show_logs:
        logger.success(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} –∑–∞–ø–∏—Å–µ–π –≤ –±–∞–∑—É —á–µ—Ä–µ–∑ ORM.")
    else:
        try:
            console.print(f"\n[bold green]üíæ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∞: +{saved_count} –∑–∞–ø–∏—Å–µ–π[/bold green]")
        except ImportError:
            print(f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∞: +{saved_count} –∑–∞–ø–∏—Å–µ–π")

    if not report_data:
        if show_logs:
            logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞.")
        return
    try:
        with open("report.json", "w", encoding="utf-8") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=4)
        if show_logs:
            logger.success("–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ report.json")
    except Exception as e:
        if show_logs:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")
        else:
            console.print(f"[red]‚ùå –û—à–∏–±–∫–∞ JSON: {e}[/red]")

    try:
        fieldnames = report_data[0].keys()
        with open("report.csv", "w", newline='', encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(report_data)
        if show_logs:
            logger.success("–§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ report.csv")
        else:
            console.print(f"\n[bold green]üíæ –û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: report.json, report.csv[/bold green]")
    except Exception as e:
        if show_logs:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è CSV: {e}")
        else:
            console.print(f"[red]‚ùå –û—à–∏–±–∫–∞ CSV: {e}[/red]")

def analyze_title_sentiment(title: str | None) -> str:
    if not title:
        return ""
    title_lower = title.lower()
    for trigger in CLICKBAIT_TRIGGERS:
        if trigger in title_lower:
            logger.debug(f"–ù–∞–π–¥–µ–Ω–æ —Ç—Ä–∏–≥–≥–µ—Ä-—Å–ª–æ–≤–æ: '{trigger}'")
            return " (–ö–ª–∏–∫–±–µ–π—Ç: –¢—Ä–∏–≥–≥–µ—Ä-—Å–ª–æ–≤–æ)"
    if '!!!' in title or '???' in title or '!?' in title:
        logger.debug("–ù–∞–π–¥–µ–Ω–∞ —á—Ä–µ–∑–º–µ—Ä–Ω–∞—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è")
        return " (–ö–ª–∏–∫–±–µ–π—Ç: –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è)"
    if title.isupper() and len(title) > 10:
        logger.debug("–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞–ø–∏—Å–∞–Ω –≤ ALL CAPS")
        return " (–ö–ª–∏–∫–±–µ–π—Ç: ALL CAPS)"
    return ""

async def fetch_and_parse_url(client: httpx.AsyncClient, url: str, semaphore: asyncio.Semaphore, show_logs: bool) -> dict:
    async with semaphore:
        if show_logs:
            logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {url}")
        else:
            console.print(f"[grey50]‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞: {urlparse(url).netloc}...[/grey50]")
        domain_rating = get_domain_rating(url)
        report_item = {
            'url': url,
            'title': None,
            'published_date': None,
            'rating': domain_rating,
            'status': 'Failed', # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
            'ai_analysis': None,
            'text_content': None
        }
        ai_score_short = ""
        try:
            response = await client.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "lxml")
            article = Article(url)
            article.set_html(response.text)
            article.parse()

            if "youtube.com" in url or "youtu.be" in url:
                report_item['ai_analysis'] = "–ü—Ä–æ–ø—É—â–µ–Ω–æ (–í–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç)"
                if show_logs: logger.info("AI –ø—Ä–æ–ø—É—â–µ–Ω (YouTube)")
            elif not article.text or len(article.text) < 100:
                report_item['ai_analysis'] = "–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
                if show_logs: logger.warning("AI –ø—Ä–æ–ø—É—â–µ–Ω (–ú–∞–ª–æ —Ç–µ–∫—Å—Ç–∞)")
            else:
                report_item['text_content'] = article.text
                if show_logs: logger.info("–û—Ç–ø—Ä–∞–≤–ª—è—é —Ç–µ–∫—Å—Ç –≤ AI...")
                ai_result = await get_ai_analyzis(article.text)
                if ai_result:
                    report_item['ai_analysis'] = ai_result
                    if show_logs: logger.success("AI –∞–Ω–∞–ª–∏–∑ –ø–æ–ª—É—á–µ–Ω!")
                    match = re.search(r'(\d{1,3}%)', ai_result)
                    if match:
                        ai_score_short = f" | AI: {match.group(1)}"
                    await asyncio.sleep(2)

            title = article.title
            if not title:
                title_tag = soup.find("title")
                h1_tag = soup.find("h1")
                title = title_tag.text.strip() if title_tag else (h1_tag.text.strip() if h1_tag else None)
            if title and show_logs:
                logger.success(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {title}")
            else:
                if show_logs: logger.warning("–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

            report_item['title'] = title
            sentiment_tag = analyze_title_sentiment(title)
            final_rating = f"{domain_rating}{sentiment_tag}{ai_score_short}"
            report_item['rating'] = final_rating

            publish_date = article.publish_date
            if publish_date:
                if isinstance(publish_date, datetime.datetime):
                    iso_date = publish_date.isoformat()
                else:
                    iso_date = str(publish_date)
                if show_logs: logger.success(f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {iso_date}")
                report_item['published_date'] = iso_date
            else:
                if show_logs: logger.debug("–ü—Ä–æ–±—É—é –Ω–∞–π—Ç–∏ –¥–∞—Ç—É –≤—Ä—É—á–Ω—É—é")
                raw_date_str = extract_date(soup)
                if raw_date_str:
                    parsed_date = dateparser.parse(str(raw_date_str))
                    if parsed_date:
                        iso_date = parsed_date.isoformat()
                        if show_logs: logger.success(f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {iso_date}")
                        report_item['published_date'] = iso_date
                    else:
                        if show_logs: logger.warning(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞ –¥–∞—Ç—ã, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å: {raw_date_str}")
                else:
                    if show_logs: logger.warning("–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            report_item['status'] = 'Success'
            if show_logs:
                logger.success(f"{final_rating}")
                if report_item['published_date']:
                    logger.success(f"–î–∞—Ç–∞: {report_item['published_date']}")
                print("\n")
            else:
                print_rich_card(report_item)

        except Exception as e:
            if show_logs:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url}: {e}")
            else:
                console.print(f"[red]‚ùå –û—à–∏–±–∫–∞ {urlparse(url).netloc}: {e}[/red]")
            report_item['status'] = f"Failed: {e}"
            print("\n")

        return report_item

async def run_parser(search_results_data, query, show_logs: bool):
    links = [item["link"] for item in search_results_data.get("items", [])]

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
    }

    semaphore = asyncio.Semaphore(3)
    tasks = []
    if not show_logs:
        console.print(f"[bold cyan]üöÄ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è {len(links)} —Å—Å—ã–ª–æ–∫...[/bold cyan]\n")
    async with httpx.AsyncClient(headers=headers, follow_redirects=True, verify=False, http2=True, trust_env=False,) as client:
        for url in links:
            tasks.append(fetch_and_parse_url(client, url, semaphore, show_logs))
        if show_logs: logger.info(f"–ó–∞–ø—É—Å–∫–∞—é {len(tasks)} –∑–∞–¥–∞—á –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ...")
        final_report_data = await asyncio.gather(*tasks)

    save_report(final_report_data, query, show_logs)
    return final_report_data


async def get_cross_check_analysis(articles_data: list ) -> str:
    valid_articles = [a for a in articles_data if a.get('text_content')]

    if len(valid_articles) < 2:
        return "‚ö†Ô∏è –î–ª—è –∫—Ä–æ—Å—Å-–∞–Ω–∞–ª–∏–∑–∞ –Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 —É—Å–ø–µ—à–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ —Å —Ç–µ–∫—Å—Ç–æ–º."

    context_text = ""
    for i, art in enumerate(valid_articles):
        text_snippet = art['text_content'][:4000]
        domain = urlparse(art['url']).netloc
        context_text += f"\n=== –ò–°–¢–û–ß–ù–ò–ö {i+1} ({domain}) ===\n{text_snippet}\n"

    prompt = f"""
    –¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –º–µ–¥–∏–∞ –∏ OSINT-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç.
    –¢–≤–æ—è –∑–∞–¥–∞—á–∞: –ø—Ä–æ–≤–µ—Å—Ç–∏ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (Cross-Check) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –Ω–∏–∂–µ —Å—Ç–∞—Ç–µ–π –æ–± –æ–¥–Ω–æ–º –∏–ª–∏ —Å—Ö–æ–∂–∏—Ö —Å–æ–±—ã—Ç–∏—è—Ö.

    –ò–°–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï:
    {context_text}

    –ó–ê–î–ê–ß–ê:
    –ù–∞–ø–∏—à–∏ —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown.

    –°–¢–†–£–ö–¢–£–†–ê –û–¢–ß–ï–¢–ê:
    1. üìù **–ö—Ä–∞—Ç–∫–∞—è —Å—É—Ç—å —Å–æ–±—ã—Ç–∏—è**: (–û —á–µ–º –≤–æ–æ–±—â–µ —Ä–µ—á—å, 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Ñ–∞–∫—Ç—ã, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã–µ –≤—Å–µ–º–∏).
    2. ‚öñÔ∏è **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞—Ä—Ä–∞—Ç–∏–≤–æ–≤**:
       - –ö–∞–∫ —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ–¥–∞—é—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é?
       - –ï—Å—Ç—å –ª–∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ–∫—Ä–∞—Å–∫–∞ (–∫—Ç–æ –æ–±–≤–∏–Ω—è–µ—Ç, –∫—Ç–æ –∑–∞—â–∏—â–∞–µ—Ç)?
    3. üîç **–ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –∏ –£–º–æ–ª—á–∞–Ω–∏—è**:
       - –í —á–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Ä–∞—Å—Ö–æ–¥—è—Ç—Å—è (—Ü–∏—Ñ—Ä—ã, –¥–∞—Ç—ã, –≤–∏–Ω–æ–≤–Ω–∏–∫–∏)?
       - –ï—Å—Ç—å –ª–∏ —Ñ–∞–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫ –≤—ã–ø—è—á–∏–≤–∞–µ—Ç, –∞ –¥—Ä—É–≥–æ–π —Å–∫—Ä—ã–≤–∞–µ—Ç?
    4. üèÜ **–í–µ—Ä–¥–∏–∫—Ç**:
       - –ö–∞–∫–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ –≤—ã–≥–ª—è–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–º –∏ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º?
       - –ï—Å—Ç—å –ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø—Ä–æ–ø–∞–≥–∞–Ω–¥—ã?

    –ü–∏—à–∏ —á–µ—Ç–∫–æ, –∏—Å–ø–æ–ª—å–∑—É–π –±—É–ª–ª–∏—Ç—ã. –ù–µ –ª–µ–π –≤–æ–¥—É.
    """

    try:
        client = genai.Client(api_key=os.getenv("GEMINI_KEY"))
        response = await client.aio.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt
        )
        if response is None or not hasattr(response, "text") or response.text is None:
            return "‚ùå –û—à–∏–±–∫–∞: AI –Ω–µ –≤–µ—Ä–Ω—É–ª —Ç–µ–∫—Å—Ç"
        return response.text

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∫—Ä–æ—Å—Å-–∞–Ω–∞–ª–∏–∑–∞: {e}")
        return f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Å—Ç–∏ –∫—Ä–æ—Å—Å-–∞–Ω–∞–ª–∏–∑: {e}"
